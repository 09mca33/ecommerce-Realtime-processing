<img width="517" height="283" alt="image" src="https://github.com/user-attachments/assets/8708be44-5787-43b4-9052-334bb70f97db" />


Real-Time Streaming Pipeline: Azure Event Hub â†’ PySpark Structured Streaming â†’ Delta Lake â†’ Power BI

This project demonstrates a complete real-time streaming data pipeline built using Azure cloud services, Apache Spark (Databricks), Delta Lake, and Power BI.
It showcases how data can be ingested from an event streaming platform, processed in real time, stored as Delta tables, and visualized instantly.

ðŸš€ Architecture Overview

Event Generator â†’ Azure Event Hub â†’ Databricks PySpark Streaming â†’ Bronze/Silver Delta Tables â†’ Power BI Dashboard

Event Generator (Python)

Simulates real-time orders (product, quantity, price, timestamp, state)

Publishes streaming messages to Azure Event Hub

Azure Event Hub

Fully managed ingestion service

Acts as a Kafka-compliant message broker

Databricks Structured Streaming (PySpark)

Reads from Event Hub using Kafka API

Cleans and transforms data

Writes data into:

Bronze Layer: raw streaming ingestion

Silver Layer: cleaned, validated, enriched data

Delta Lake

Stores both bronze and silver tables

Provides reliability, schema enforcement, ACID transactions

Power BI

Connects to Delta Lake (ADLS Gen2 or Databricks SQL Endpoint)

Visualizes real-time metrics such as:

Total sales

Orders by state

Product performance

Time-based KPIs



ecommerce-streaming/
â”‚
â”œâ”€â”€ simulator/
â”‚   â””â”€â”€ order-generator.py        # Python script to push events to Event Hub
â”‚
â”œâ”€â”€ databricks/
â”‚   â”œâ”€â”€ bronze-stream.py          # Streaming read from Event Hub â†’ Bronze
â”‚   â”œâ”€â”€ silver-transform.py       # Streaming transforms â†’ Silver
â”‚   â””â”€â”€ utils/                    # Event Hub configs, schema
â”‚
â”œâ”€â”€ powerbi/
â”‚   â””â”€â”€ dashboard.pbix            # Power BI report (optional)
â”‚
â””â”€â”€ README.md
